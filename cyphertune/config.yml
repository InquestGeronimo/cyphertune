LoraConfig:
  r: 8
  lora_alpha: 16
  target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj
    - gate_proj
    - up_proj
    - down_proj
    - lm_head
  bias: none
  lora_dropout: 0.05
  task_type: CAUSAL_LM

TrainerConfig:
  output_dir: ./output
  warmup_steps: 1
  per_device_train_batch_size: 32
  gradient_accumulation_steps: 1
  gradient_checkpointing: true
  max_steps: 1000
  learning_rate: 1e-4
  bf16: true
  optim: paged_adamw_8bit
  logging_dir: ./logs
  save_strategy: steps
  save_steps: 25
  evaluation_strategy: steps
  eval_steps: 50
  do_eval: true
  report_to: wandb
  remove_unused_columns: true
  run_name: cypher-train-1